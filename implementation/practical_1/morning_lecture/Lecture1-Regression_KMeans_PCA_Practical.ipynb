{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "650nFK-U8cBp"
   },
   "source": [
    "# **ACSE Module 8 - Practical 1**\n",
    "\n",
    "# Regression, k-means and PCA\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8mwBbB2BNrbp"
   },
   "source": [
    "## Objectives\n",
    "\n",
    "The aim of today is to give you the practical skills that will enable you to **successfully** apply linear and logistic regression, k-means, and PCA to real world datasets. \n",
    "\n",
    "1. Clean and pre-process data for linear and logistic regression\n",
    "2. Conduct exploratory data analysis by visualisation\n",
    "3. Create and evaluate linear and logistic regression models\n",
    "4. Understand the difference between using regression for inference or prediction\n",
    "5. Be aware of the 4 key assumptions behind linear regression\n",
    "6. Perform clustering using k-means \n",
    "7. Utilize PCA to deal with high dimensional data\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m1VCv2Or83hR"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2068,
     "status": "ok",
     "timestamp": 1619356376090,
     "user": {
      "displayName": "George Stronge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjB4k01tg6PbeY45zroI_SbiF8yAjZ2njmJtl691w=s64",
      "userId": "17668309495408135664"
     },
     "user_tz": -60
    },
    "id": "qtR8wK4s6Tp9",
    "outputId": "be8e6788-6834-41e8-fd3d-41b7161e3ca0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import pandas as pd # for manipulating tabular data\n",
    "import matplotlib.pyplot as plt # for visualisation\n",
    "import seaborn as sns # for user friendly visualisation\n",
    "import numpy as np # for numerical python functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0yzKWmnZ-u9R"
   },
   "source": [
    "# 1. Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmq7FeTh_O6C"
   },
   "source": [
    "I know that you’ve always dreamed of dominating the housing market. Until now, that was impossible. But with this limited offer you can… got a bit sidetracked there.\n",
    "\n",
    "We will work with the housing prices dataset from [Kaggle](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) which was compiled by Dean De Cock for use in data science education. It's an incredible alternative for data scientists looking for a modernized and expanded version of the often cited Boston Housing dataset. The meanings of the columns are described [here](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ZsZJ3I-CKaB"
   },
   "source": [
    "## 1.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 2067,
     "status": "ok",
     "timestamp": 1619356376092,
     "user": {
      "displayName": "George Stronge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjB4k01tg6PbeY45zroI_SbiF8yAjZ2njmJtl691w=s64",
      "userId": "17668309495408135664"
     },
     "user_tz": -60
    },
    "id": "ViVb-tKnCIHj"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression # implementation of linear regression\n",
    "from sklearn.model_selection import train_test_split # for creating a train and test set\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score # for evaluating our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TxpbF7D4_1oA"
   },
   "source": [
    "## 1.2 Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TRByKDIC8GOZ"
   },
   "outputs": [],
   "source": [
    "# download the data\n",
    "!wget https://raw.githubusercontent.com/acse-2020/ACSE-8/main/implementation/practical_1/morning_lecture/Houseprices.csv?token=ABNZJP5IAN37ENU4WBELAV3ASDFJY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 469,
     "status": "ok",
     "timestamp": 1619356376729,
     "user": {
      "displayName": "George Stronge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjB4k01tg6PbeY45zroI_SbiF8yAjZ2njmJtl691w=s64",
      "userId": "17668309495408135664"
     },
     "user_tz": -60
    },
    "id": "yOuVH6TH82Rh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 483,
     "status": "ok",
     "timestamp": 1619356378394,
     "user": {
      "displayName": "George Stronge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjB4k01tg6PbeY45zroI_SbiF8yAjZ2njmJtl691w=s64",
      "userId": "17668309495408135664"
     },
     "user_tz": -60
    },
    "id": "0xamm5EO821g"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oHdkGRcUBH2I"
   },
   "source": [
    "## 1.3 Dimensions and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 503,
     "status": "ok",
     "timestamp": 1619356388682,
     "user": {
      "displayName": "George Stronge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjB4k01tg6PbeY45zroI_SbiF8yAjZ2njmJtl691w=s64",
      "userId": "17668309495408135664"
     },
     "user_tz": -60
    },
    "id": "x1MgpoySAWNC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 501,
     "status": "ok",
     "timestamp": 1619356394892,
     "user": {
      "displayName": "George Stronge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjB4k01tg6PbeY45zroI_SbiF8yAjZ2njmJtl691w=s64",
      "userId": "17668309495408135664"
     },
     "user_tz": -60
    },
    "id": "so8WL0alB18g"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B137tn1XCNrv"
   },
   "source": [
    "Reminder: The meanings of these features are described [here](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ITuyLxakG4Gs"
   },
   "source": [
    "## 1.4 Data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 481,
     "status": "ok",
     "timestamp": 1619356398137,
     "user": {
      "displayName": "George Stronge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjB4k01tg6PbeY45zroI_SbiF8yAjZ2njmJtl691w=s64",
      "userId": "17668309495408135664"
     },
     "user_tz": -60
    },
    "id": "Fhf-t4-YG85r"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRynuKvNIfZ-"
   },
   "source": [
    "### One-hot encoding\n",
    "\n",
    "Some of these data types are clearly categorical. In order to use categorical features in ML models we would have to [one-hot encode them](https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/). This is easy enough to do using the `pd.get_dummies()` function. For more information, [here](https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html#) is the documentation. For now, we will simply deal with the numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 699,
     "status": "ok",
     "timestamp": 1619356402041,
     "user": {
      "displayName": "George Stronge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjB4k01tg6PbeY45zroI_SbiF8yAjZ2njmJtl691w=s64",
      "userId": "17668309495408135664"
     },
     "user_tz": -60
    },
    "id": "5iJBmpZ4xg6x"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wZmb5vOWEySg"
   },
   "source": [
    "## 1.5 How many missing values do we have in each column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 463,
     "status": "ok",
     "timestamp": 1619356410384,
     "user": {
      "displayName": "George Stronge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjB4k01tg6PbeY45zroI_SbiF8yAjZ2njmJtl691w=s64",
      "userId": "17668309495408135664"
     },
     "user_tz": -60
    },
    "id": "2WnMMuzQB9vc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 876,
     "status": "ok",
     "timestamp": 1619356415588,
     "user": {
      "displayName": "George Stronge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjB4k01tg6PbeY45zroI_SbiF8yAjZ2njmJtl691w=s64",
      "userId": "17668309495408135664"
     },
     "user_tz": -60
    },
    "id": "biJEgxIWC8IL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yyooN29TBISr"
   },
   "source": [
    "### Data imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "--qh7gUC83IZ"
   },
   "source": [
    "It may be worth taking the time to deal with missing values so as to best squeeze as much information as we can from the dataset. However, the columns with a significant number of missing values (> 40%) are probably not worth considering and it would be wise to avoid using these features in our regression model.\n",
    "\n",
    "For the other features there are a number of strategies we could take to replace missing values with a sensible estimate. For a numerical feature, the most simple of these strategies could be to replace missing values with the median value for that feature. For instance, we could run the following line of code to impute the missing values in the 'MasVnrArea' feature:\n",
    "\n",
    "    houses['MasVnrArea'].fillna(houses['MasVnrArea'].median(), inplace=True)\n",
    "\n",
    " If you are dealing with a categorical feature, you could replace missing values with the mode instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 756,
     "status": "ok",
     "timestamp": 1619356419612,
     "user": {
      "displayName": "George Stronge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjB4k01tg6PbeY45zroI_SbiF8yAjZ2njmJtl691w=s64",
      "userId": "17668309495408135664"
     },
     "user_tz": -60
    },
    "id": "uM-utwKdGyTm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 341,
     "status": "ok",
     "timestamp": 1619356421590,
     "user": {
      "displayName": "George Stronge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjB4k01tg6PbeY45zroI_SbiF8yAjZ2njmJtl691w=s64",
      "userId": "17668309495408135664"
     },
     "user_tz": -60
    },
    "id": "DxVg8sJYHRdU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 924,
     "status": "ok",
     "timestamp": 1619356423738,
     "user": {
      "displayName": "George Stronge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjB4k01tg6PbeY45zroI_SbiF8yAjZ2njmJtl691w=s64",
      "userId": "17668309495408135664"
     },
     "user_tz": -60
    },
    "id": "73PErcnzIlkz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WDp9i34PKaK-"
   },
   "source": [
    "## 1.6 Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 955,
     "status": "ok",
     "timestamp": 1619356427519,
     "user": {
      "displayName": "George Stronge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjB4k01tg6PbeY45zroI_SbiF8yAjZ2njmJtl691w=s64",
      "userId": "17668309495408135664"
     },
     "user_tz": -60
    },
    "id": "Dco0-MxeLKT-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 477,
     "status": "ok",
     "timestamp": 1619356430318,
     "user": {
      "displayName": "George Stronge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjB4k01tg6PbeY45zroI_SbiF8yAjZ2njmJtl691w=s64",
      "userId": "17668309495408135664"
     },
     "user_tz": -60
    },
    "id": "Brp5xWzlEwig"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 575,
     "status": "ok",
     "timestamp": 1619356433981,
     "user": {
      "displayName": "George Stronge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjB4k01tg6PbeY45zroI_SbiF8yAjZ2njmJtl691w=s64",
      "userId": "17668309495408135664"
     },
     "user_tz": -60
    },
    "id": "O7fOYykMJlxc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 311,
     "status": "ok",
     "timestamp": 1619356436480,
     "user": {
      "displayName": "George Stronge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjB4k01tg6PbeY45zroI_SbiF8yAjZ2njmJtl691w=s64",
      "userId": "17668309495408135664"
     },
     "user_tz": -60
    },
    "id": "onlXnng2G3Vu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 467,
     "status": "ok",
     "timestamp": 1619356440518,
     "user": {
      "displayName": "George Stronge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjB4k01tg6PbeY45zroI_SbiF8yAjZ2njmJtl691w=s64",
      "userId": "17668309495408135664"
     },
     "user_tz": -60
    },
    "id": "-_xRZfBKJ1zK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 715,
     "status": "ok",
     "timestamp": 1619356445251,
     "user": {
      "displayName": "George Stronge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjB4k01tg6PbeY45zroI_SbiF8yAjZ2njmJtl691w=s64",
      "userId": "17668309495408135664"
     },
     "user_tz": -60
    },
    "id": "XslZWdZjH2oE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 479,
     "status": "ok",
     "timestamp": 1619356449454,
     "user": {
      "displayName": "George Stronge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjB4k01tg6PbeY45zroI_SbiF8yAjZ2njmJtl691w=s64",
      "userId": "17668309495408135664"
     },
     "user_tz": -60
    },
    "id": "b0frPUQrIcpX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "irTw61AFSifM"
   },
   "source": [
    "### Multicolinearity\n",
    "\n",
    "If you are using regression for inference rather than as a predictive tool, it is important to check for [muilticolinearity](https://en.wikipedia.org/wiki/Multicollinearity#Definition). This is when your features are highly correlated with one-another (i.e. the magnitude of the correlation coefficient between them is 0.6 or more). It can cause the regression problem to become ill-conditioned, where coefficients/parameters change dramatically in response to small changes in the data. In practice, this results in incorrect and misleading coefficients/parameters. More on inference later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fC3zt8dnSjae"
   },
   "source": [
    "## 1.7 Select features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 936,
     "status": "ok",
     "timestamp": 1619356482581,
     "user": {
      "displayName": "George Stronge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjB4k01tg6PbeY45zroI_SbiF8yAjZ2njmJtl691w=s64",
      "userId": "17668309495408135664"
     },
     "user_tz": -60
    },
    "id": "VIf0_2qR8HqG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 528,
     "status": "ok",
     "timestamp": 1619356485234,
     "user": {
      "displayName": "George Stronge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjB4k01tg6PbeY45zroI_SbiF8yAjZ2njmJtl691w=s64",
      "userId": "17668309495408135664"
     },
     "user_tz": -60
    },
    "id": "C_95_9DRCGgH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DCzzW3jbRtNV"
   },
   "source": [
    "## 1.8 Train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 462,
     "status": "ok",
     "timestamp": 1619356489121,
     "user": {
      "displayName": "George Stronge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjB4k01tg6PbeY45zroI_SbiF8yAjZ2njmJtl691w=s64",
      "userId": "17668309495408135664"
     },
     "user_tz": -60
    },
    "id": "fISjT9UeEoDN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17764,
     "status": "ok",
     "timestamp": 1619353607480,
     "user": {
      "displayName": "George Stronge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjB4k01tg6PbeY45zroI_SbiF8yAjZ2njmJtl691w=s64",
      "userId": "17668309495408135664"
     },
     "user_tz": -60
    },
    "id": "br-51Zl1FdIl",
    "outputId": "f1e3e863-0766-4a0f-b9b3-41b600e204f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (1096, 9)\n",
      "y_train: (1096,)\n",
      "X_test: (275, 9)\n",
      "y_test: (275,)\n"
     ]
    }
   ],
   "source": [
    "# Check the shapes\n",
    "print('X_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('X_test:', X_test.shape)\n",
    "print('y_test:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z5fagjeVR_p4"
   },
   "source": [
    "## 1.9 Creating our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 742,
     "status": "ok",
     "timestamp": 1619356492619,
     "user": {
      "displayName": "George Stronge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjB4k01tg6PbeY45zroI_SbiF8yAjZ2njmJtl691w=s64",
      "userId": "17668309495408135664"
     },
     "user_tz": -60
    },
    "id": "6wMF0ielGAwF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WPAl1VmcVnrx"
   },
   "source": [
    "## 1.10 Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JRKspfzzWFAc"
   },
   "source": [
    "In order to evaluate how effective our model is for predictive modelling we need to see how well it performs on unseen data; the test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 800,
     "status": "ok",
     "timestamp": 1619356496698,
     "user": {
      "displayName": "George Stronge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjB4k01tg6PbeY45zroI_SbiF8yAjZ2njmJtl691w=s64",
      "userId": "17668309495408135664"
     },
     "user_tz": -60
    },
    "id": "H4ISEaHdKUYS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 736,
     "status": "ok",
     "timestamp": 1619356498291,
     "user": {
      "displayName": "George Stronge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjB4k01tg6PbeY45zroI_SbiF8yAjZ2njmJtl691w=s64",
      "userId": "17668309495408135664"
     },
     "user_tz": -60
    },
    "id": "XgvtjucCdPUq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3J8OJEQPgood"
   },
   "source": [
    "Here we have used both the mean absolute error and the root mean squared error. The mean absolute error is more robust to outliers than the root mean squared error as it doesn't involve squaring your residuals. However, using the root mean squared error may be preferable if you want to make sure that your models have taken outliers into account. You can read more about these error metrics [here](https://www.dataquest.io/blog/understanding-regression-error-metrics/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7HsRDSQmLI7E"
   },
   "outputs": [],
   "source": [
    "# evaluation can also be done visually by plotting predictions vs the true target\n",
    "# across the entire dataset\n",
    "preds = model.predict(X)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(y, preds, alpha=0.4, linewidths=1, edgecolors='white')\n",
    "plt.plot(y, y, color='darkred', alpha=0.5)\n",
    "plt.xlabel('Real Sale Price')\n",
    "plt.ylabel('Predicted Sale Price')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bcNoY7ZImicJ"
   },
   "source": [
    "From the above plot we can see that, for higher sale prices (>400,000), our model is consistently underestimating the true sale price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06FIP5RCprah"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gKvCz61qy_dv"
   },
   "source": [
    "## **Challenge 1**\n",
    "\n",
    "Incorporate categorical features in your regression model. Does this improve your predictions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L6OZkFHdpoeu"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SxMZYBhCqYlF"
   },
   "source": [
    "## 1.11 Inference "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQXZIDgGxOZr"
   },
   "source": [
    "So far we have been using linear regression purely as a predictive tool. However, one of the main advantageous of linear regression is that it is incredibely useful for inference. This is where we use our model to try and understand how the target variable was generated from our features, rather than just predicting it. You can read more about the differences between prediction and inference [here](https://www.datascienceblog.net/post/commentary/inference-vs-prediction/).\n",
    "\n",
    "The reason linear regression is great for inference is that we can easily interpret the meaning of the parameters/coefficients of our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 650,
     "status": "ok",
     "timestamp": 1619356526556,
     "user": {
      "displayName": "George Stronge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjB4k01tg6PbeY45zroI_SbiF8yAjZ2njmJtl691w=s64",
      "userId": "17668309495408135664"
     },
     "user_tz": -60
    },
    "id": "DenbvVJgNb75"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 472,
     "status": "ok",
     "timestamp": 1619356530691,
     "user": {
      "displayName": "George Stronge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjB4k01tg6PbeY45zroI_SbiF8yAjZ2njmJtl691w=s64",
      "userId": "17668309495408135664"
     },
     "user_tz": -60
    },
    "id": "OCrHAXVsN5i2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 610,
     "status": "ok",
     "timestamp": 1619356534152,
     "user": {
      "displayName": "George Stronge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjB4k01tg6PbeY45zroI_SbiF8yAjZ2njmJtl691w=s64",
      "userId": "17668309495408135664"
     },
     "user_tz": -60
    },
    "id": "a4z4Sqg1zGUj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 518,
     "status": "ok",
     "timestamp": 1619356536188,
     "user": {
      "displayName": "George Stronge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjB4k01tg6PbeY45zroI_SbiF8yAjZ2njmJtl691w=s64",
      "userId": "17668309495408135664"
     },
     "user_tz": -60
    },
    "id": "kjAoMftNontv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZxU8NZ8mxVJi"
   },
   "source": [
    "From this table we could deduce that, for a unit increase in the `OverallQual` feature, we would expect an increase of $23057 in the target variable `SalePrice`. \n",
    "\n",
    "However, you may have noticed that the variables `GarageCars` and `GarageArea` are essentially two different measures of the same thing and are therefore highly correlated with each other. This means that our model will be unable to find reliable estimates for their corresponding coefficients due to [multicolinearity](https://en.wikipedia.org/wiki/Multicollinearity). To mitigate multicolinearity we could simply remove one of these features, or perhaps they could be combined into a new feature. It is likely that some of the other features we are using also exhibit multicolinearity and this should be explored further or we will be unable to trust these coefficients/parameters.\n",
    "\n",
    "It is also worth noting that, when using regression for inference, the [statsmodels library](https://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.OLS.html) is generally prefered to sklearn as it readily provides statistical tests on your model parameters/coefficients. This is really useful as it allows us to see if all of our features are actually adding something valueble to our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U2bmw0k8q9jw"
   },
   "source": [
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IOUaN2EAJN3g"
   },
   "source": [
    "## **Challenge 2**\n",
    "\n",
    "Take reasonable steps to mitigate multicolinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "09lDTWQOq_fZ"
   },
   "source": [
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uPOl-HUqQJuT"
   },
   "source": [
    "## 1.12 Assumptions behind linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_PMMjdVb1Ing"
   },
   "source": [
    "When using regression for predictive modelling, most practitioners don't worry too much about the assumptions behind linear regression as long as the performance on the test set is satisfactory. From my experience it is still a good idea to check them as they may inform us on what we can do to get even better predictions. \n",
    "\n",
    "However, if you are using regression for inference purposes, it is absolutely crucial to validate these assumptions or you may end up with incorrect and misleading interpretations. The 4 main assumptions are as follows: \n",
    "\n",
    "1.   **Linear relationship**: There exists a linear relationship between the independent variable, x, and the dependent variable, y.\n",
    "2.   **Independence**: The residuals are independent. In particular, there is no correlation between consecutive residuals in time series data.\n",
    "3.   **Homoscedasticity**: The residuals have constant variance at every level of x.\n",
    "4.   **Normality**: The residuals are normally distributed.\n",
    "\n",
    "For more information, as well as a discussion on what to do if your model does not meet these assumptions, please checkout [this](https://www.statology.org/linear-regression-assumptions/) article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fjTCwXIErA1N"
   },
   "source": [
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nkPnvEOSP9ic"
   },
   "source": [
    "## **Challenge 3**\n",
    "\n",
    "Check whether we have met each of the 4 assumptions behind linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tcVnVwAGkbxT"
   },
   "source": [
    "## **Challenge 4**\n",
    "\n",
    "If we have not met some of these assumption, is there anything (preferably something simple) that we can do to make sure we do satisfy them? If so, try to implement it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0KZ4B1Ksrhb1"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0rVp8q0Q2S5o"
   },
   "source": [
    "# 2. K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rN-WKsmvn2qa"
   },
   "source": [
    "Let us now turn our attention to clustering and see what can be gained by applying the K-Means algorithm to this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hvE65ibEoLaN"
   },
   "source": [
    "## 2.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 508,
     "status": "ok",
     "timestamp": 1619356546216,
     "user": {
      "displayName": "George Stronge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjB4k01tg6PbeY45zroI_SbiF8yAjZ2njmJtl691w=s64",
      "userId": "17668309495408135664"
     },
     "user_tz": -60
    },
    "id": "Q54xL3PdQM0_"
   },
   "outputs": [],
   "source": [
    "# to scale our data so that we can perform \"sensible\" clustering\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "# to provide an implementation of the k-means algorithm\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# to provide an implementation of PCA for reducing the dimensionality of our data\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xi91XjQFoNzH"
   },
   "source": [
    "## 2.2 Define and fit the k-means model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 552,
     "status": "ok",
     "timestamp": 1619356549264,
     "user": {
      "displayName": "George Stronge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjB4k01tg6PbeY45zroI_SbiF8yAjZ2njmJtl691w=s64",
      "userId": "17668309495408135664"
     },
     "user_tz": -60
    },
    "id": "pVlXBQFB25dC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E3QIc1qMr-LD"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6VfAP_vwdPZK"
   },
   "source": [
    "## **Challenge 5**\n",
    "\n",
    "How do you know that you have the correct hyperparemeter k when using k-means? Try using the [Elbow method](https://en.wikipedia.org/wiki/Elbow_method_(clustering)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dTv4K9-bsAqg"
   },
   "source": [
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 488,
     "status": "ok",
     "timestamp": 1619356553131,
     "user": {
      "displayName": "George Stronge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjB4k01tg6PbeY45zroI_SbiF8yAjZ2njmJtl691w=s64",
      "userId": "17668309495408135664"
     },
     "user_tz": -60
    },
    "id": "AUrGvYS73JLd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ka8Ye7UYpwXD"
   },
   "source": [
    "## 2.3 PCA for visualizing our clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2fFHdwPqqASQ"
   },
   "source": [
    "One of the main challenges of clustering in high dimensional spaces is how we actually visualize our clusters. PCA allows us to reduce this 9-dimensional space down to just 2 dimensions. This allows us to easily view and inspect our clusters in a simple 2D scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 473,
     "status": "ok",
     "timestamp": 1619356559248,
     "user": {
      "displayName": "George Stronge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjB4k01tg6PbeY45zroI_SbiF8yAjZ2njmJtl691w=s64",
      "userId": "17668309495408135664"
     },
     "user_tz": -60
    },
    "id": "l1yJpgriq88B"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zv8Key_v2oKA"
   },
   "outputs": [],
   "source": [
    "# lets visualize our data in our reduced 2-dimensional space\n",
    "plt.figure  (figsize=(12, 5))\n",
    "plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1], c=y_train, alpha=0.7, linewidths=1, \n",
    "            edgecolors='white', cmap='inferno')\n",
    "plt.xlabel('Principal component 1')\n",
    "plt.ylabel('Principal componene 2')\n",
    "sns.despine()\n",
    "plt.colorbar(label='Sale Price ($)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Mk11xhk266c"
   },
   "outputs": [],
   "source": [
    "# lets visualize our data in our reduced 2-dimensional space\n",
    "plt.figure(figsize=(10, 5))\n",
    "plot = plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1], c=X_train['cluster'], linewidths=1,\n",
    "                   edgecolors='white', cmap='viridis_r')\n",
    "plt.xlabel('Principal component 1')\n",
    "plt.ylabel('Principal componene 2')\n",
    "plt.legend(*plot.legend_elements(), loc=\"lower right\", title=\"Clusters\")\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 784,
     "status": "ok",
     "timestamp": 1619356572382,
     "user": {
      "displayName": "George Stronge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjB4k01tg6PbeY45zroI_SbiF8yAjZ2njmJtl691w=s64",
      "userId": "17668309495408135664"
     },
     "user_tz": -60
    },
    "id": "S9jr4nvir7Gd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gttl4AeK2hbD"
   },
   "outputs": [],
   "source": [
    "# it's often easier to visualize these components as a heatmap\n",
    "plt.figure(figsize=(3,7))\n",
    "sns.heatmap(pd.DataFrame(pca.components_,columns=X_train.drop('cluster', axis=1).columns).transpose(), \n",
    "            cmap='RdBu_r', vmin=-1, vmax=1, square=True)\n",
    "plt.xlabel('Principal component')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('PCA components')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qo34fACj-zge"
   },
   "source": [
    "From the above plots we can hypothesize that the first principal component is correlated with `SalePrice` and the second principal component seems to be correlated with general comfort and also the year built (it will be high if it's an old house). Using this information we could now attempt to interpret our clusters in our reduced 2 dimensional space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6WvqtO-3__Y3"
   },
   "source": [
    "## 2.4 Enhancing our original regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oYqM5RVYAR07"
   },
   "source": [
    "Ok, so we have created some clusters, applied PCA to visualize them and then thought a little about interpretation... So what? Well, we could now try to use this new insight (clusters) we have mined from our data in order to enhance our linear regressions predictive performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 528,
     "status": "ok",
     "timestamp": 1619356630244,
     "user": {
      "displayName": "George Stronge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjB4k01tg6PbeY45zroI_SbiF8yAjZ2njmJtl691w=s64",
      "userId": "17668309495408135664"
     },
     "user_tz": -60
    },
    "id": "6D2HseLE5zca"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 401,
     "status": "ok",
     "timestamp": 1619356632449,
     "user": {
      "displayName": "George Stronge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjB4k01tg6PbeY45zroI_SbiF8yAjZ2njmJtl691w=s64",
      "userId": "17668309495408135664"
     },
     "user_tz": -60
    },
    "id": "th-2tYgC3aWm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 320,
     "status": "ok",
     "timestamp": 1619356634537,
     "user": {
      "displayName": "George Stronge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjB4k01tg6PbeY45zroI_SbiF8yAjZ2njmJtl691w=s64",
      "userId": "17668309495408135664"
     },
     "user_tz": -60
    },
    "id": "xwX-STpO4e8s"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 478,
     "status": "ok",
     "timestamp": 1619356637975,
     "user": {
      "displayName": "George Stronge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjB4k01tg6PbeY45zroI_SbiF8yAjZ2njmJtl691w=s64",
      "userId": "17668309495408135664"
     },
     "user_tz": -60
    },
    "id": "svcoWAIh4UF7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9L2--5425N2f"
   },
   "outputs": [],
   "source": [
    "# evaluate how good these predictions are using mae and rmse\n",
    "mae = mean_absolute_error(y_test, preds_test)\n",
    "rmse = mean_squared_error(y_test, preds_test, squared=False)\n",
    "\n",
    "print('MAE:', mae)\n",
    "print('RMSE:', rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MG7AK2JMB_tS"
   },
   "source": [
    "This is a substantial improvement on our previous model. It seems that using K-means as a pre-processing step can sometimes be an effective strategy when building predictive models! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aYa032vAFyAK"
   },
   "source": [
    "# 3. Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nt4uIburJQ1X"
   },
   "source": [
    "We will now take a look at logistic regression for the binary classification problem of predicting with a tumour is malignant or benign. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UjU31h4kKR7-"
   },
   "source": [
    "## 3.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HAq7v8vLKUUQ"
   },
   "outputs": [],
   "source": [
    "# to provide an implementation of logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# to provide an implementation of the accuracy metric\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# import dataset loader\n",
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jYSXENEcJlcr"
   },
   "source": [
    "## 3.2 Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 501,
     "status": "ok",
     "timestamp": 1619356658787,
     "user": {
      "displayName": "George Stronge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjB4k01tg6PbeY45zroI_SbiF8yAjZ2njmJtl691w=s64",
      "userId": "17668309495408135664"
     },
     "user_tz": -60
    },
    "id": "sKhDr3in3Ajo"
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "data = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 468,
     "status": "ok",
     "timestamp": 1619356660772,
     "user": {
      "displayName": "George Stronge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjB4k01tg6PbeY45zroI_SbiF8yAjZ2njmJtl691w=s64",
      "userId": "17668309495408135664"
     },
     "user_tz": -60
    },
    "id": "lTPs25UDEL8q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 520,
     "status": "ok",
     "timestamp": 1619356669536,
     "user": {
      "displayName": "George Stronge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjB4k01tg6PbeY45zroI_SbiF8yAjZ2njmJtl691w=s64",
      "userId": "17668309495408135664"
     },
     "user_tz": -60
    },
    "id": "H7eTiNqVEZz3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 346,
     "status": "ok",
     "timestamp": 1619356671531,
     "user": {
      "displayName": "George Stronge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjB4k01tg6PbeY45zroI_SbiF8yAjZ2njmJtl691w=s64",
      "userId": "17668309495408135664"
     },
     "user_tz": -60
    },
    "id": "tQECfg2H3QQb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ClMCUhjsLip5"
   },
   "source": [
    "## 3.3 Pre-processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 335,
     "status": "ok",
     "timestamp": 1619356673644,
     "user": {
      "displayName": "George Stronge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjB4k01tg6PbeY45zroI_SbiF8yAjZ2njmJtl691w=s64",
      "userId": "17668309495408135664"
     },
     "user_tz": -60
    },
    "id": "gXLvzi9wKngY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fbZMVhd9moQ1"
   },
   "source": [
    "### Imbalanced data\n",
    "\n",
    "Checking that your target variable is balanced is really important. If it is not balanced, accuracy could potentially be a misleading metric to use when it comes to evaluating your model. For instance, if only 2% of your data was malignant and 98% of your data was benign, you could achieve 98% accuracy simply by predicting benign every time, regardless of what your features are telling you! \n",
    "\n",
    "Potentially solutions include the use of additional metrics such as [precision, recall](https://en.wikipedia.org/wiki/Precision_and_recall) and the [f1-score](https://en.wikipedia.org/wiki/F-score), as well as resampling your data to counteract the imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 487,
     "status": "ok",
     "timestamp": 1619356677882,
     "user": {
      "displayName": "George Stronge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjB4k01tg6PbeY45zroI_SbiF8yAjZ2njmJtl691w=s64",
      "userId": "17668309495408135664"
     },
     "user_tz": -60
    },
    "id": "dZdvxVGAEPbs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 609,
     "status": "ok",
     "timestamp": 1619356679832,
     "user": {
      "displayName": "George Stronge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjB4k01tg6PbeY45zroI_SbiF8yAjZ2njmJtl691w=s64",
      "userId": "17668309495408135664"
     },
     "user_tz": -60
    },
    "id": "sJC_2BQUJtr3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 807,
     "status": "ok",
     "timestamp": 1619356691167,
     "user": {
      "displayName": "George Stronge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjB4k01tg6PbeY45zroI_SbiF8yAjZ2njmJtl691w=s64",
      "userId": "17668309495408135664"
     },
     "user_tz": -60
    },
    "id": "ddU0zb90LUuv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dK5TNP7lJK4o"
   },
   "source": [
    "## 3.4 Finding the top predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 528,
     "status": "ok",
     "timestamp": 1619356695929,
     "user": {
      "displayName": "George Stronge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjB4k01tg6PbeY45zroI_SbiF8yAjZ2njmJtl691w=s64",
      "userId": "17668309495408135664"
     },
     "user_tz": -60
    },
    "id": "Sb9WuvauGCgO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4awphTDOMy1z"
   },
   "outputs": [],
   "source": [
    "# visualize accuracy vs feature used for classification\n",
    "plt.figure(figsize(6,9))\n",
    "sns.barplot(a, X_train.columns, color='black')\n",
    "plt.xticks(np.arange(0,1.05,0.05))\n",
    "plt.xlim(0.5,1)\n",
    "plt.grid()\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Mgd07ltQNoB"
   },
   "source": [
    "So it would appear that `worst radius` and `worst area` yield the best accuracy classifiers and are perhaps the best predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VM-i7jn6Qx5i"
   },
   "outputs": [],
   "source": [
    "# visualize worst radius vs worst area with color as the target\n",
    "plot = plt.scatter(X_train['worst radius'], X_train['worst perimeter'], c=y_train,\n",
    "                   linewidths=1, edgecolors='white', cmap='RdYlGn')\n",
    "plt.legend(*plot.legend_elements(),\n",
    "                    loc=\"upper left\", title=\"Classes\")\n",
    "plt.xlabel('worst radius')\n",
    "plt.ylabel('worst perimeter')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mhtYYrw78hl1"
   },
   "source": [
    "The data looks very easily seperable along these dimensions. If only we could draw a horizontal line across the middle of this plot, we could guess that everything above the line is class 0 and everything below is class 1. This is what Logistic Regression does. Note that if we did want to use logistic regression for inference on these features we would likely run into difficulties as they are extremely colinear ([multicolinearity](https://en.wikipedia.org/wiki/Multicollinearity))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QT-QYnRV9-5X"
   },
   "source": [
    "## 3.5 Creating our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 543,
     "status": "ok",
     "timestamp": 1619356735237,
     "user": {
      "displayName": "George Stronge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjB4k01tg6PbeY45zroI_SbiF8yAjZ2njmJtl691w=s64",
      "userId": "17668309495408135664"
     },
     "user_tz": -60
    },
    "id": "GyRgk5s9-UYZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 678,
     "status": "ok",
     "timestamp": 1619356737889,
     "user": {
      "displayName": "George Stronge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjB4k01tg6PbeY45zroI_SbiF8yAjZ2njmJtl691w=s64",
      "userId": "17668309495408135664"
     },
     "user_tz": -60
    },
    "id": "Ta4cSuDu--oW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nq7gVMdy_xeT"
   },
   "source": [
    "## 3.6 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 486,
     "status": "ok",
     "timestamp": 1619356742415,
     "user": {
      "displayName": "George Stronge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjB4k01tg6PbeY45zroI_SbiF8yAjZ2njmJtl691w=s64",
      "userId": "17668309495408135664"
     },
     "user_tz": -60
    },
    "id": "Z3Ko9smp_mLn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "omBG6aG7BABv"
   },
   "source": [
    "## 3.7 Visualise the decision boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jL59_Y-qFeUj"
   },
   "source": [
    "A bit of background on deriving and plotting the decision boundary for logistic regression can be found [here](https://scipython.com/blog/plotting-the-decision-boundary-of-a-logistic-regression-model/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zA7Nxe3WA-9O"
   },
   "outputs": [],
   "source": [
    "# obtain model parameters/coefficients\n",
    "b = clf1.intercept_[0]\n",
    "w1, w2 = clf1.coef_.T\n",
    "\n",
    "# solve for the intercept and gradient of the decision boundary\n",
    "c = -b/w2\n",
    "m = -w1/w2\n",
    "\n",
    "# visualise the decision boundary\n",
    "xmin, xmax = 5, 37\n",
    "ymin, ymax = 48, 255\n",
    "xd = np.array([xmin, xmax])\n",
    "yd = m*xd + c\n",
    "plt.plot(xd, yd, 'k', lw=1, ls='--')\n",
    "plt.fill_between(xd, yd, ymin, color='tab:green', alpha=0.2)\n",
    "plt.fill_between(xd, yd, ymax, color='tab:purple', alpha=0.2)\n",
    "plot = plt.scatter(X_train['worst radius'], X_train['worst perimeter'], c=y_train,\n",
    "                   linewidths=1, edgecolors='white', cmap='RdYlGn')\n",
    "plt.legend(*plot.legend_elements(),\n",
    "                    loc=\"upper left\", title=\"Classes\")\n",
    "plt.xlabel('worst radius')\n",
    "plt.ylabel('worst perimeter')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AprdsZNYHuWn"
   },
   "source": [
    "## 3.8 Can PCA do any better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6C-LsIiPIJCr"
   },
   "source": [
    "Rather than selecting our two most powerful predictive features, we could also try to use all 30 features but create a two-component PCA projection of them. Then we can create a logistic classifier that will use these two components as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1275,
     "status": "ok",
     "timestamp": 1619353747694,
     "user": {
      "displayName": "George Stronge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjB4k01tg6PbeY45zroI_SbiF8yAjZ2njmJtl691w=s64",
      "userId": "17668309495408135664"
     },
     "user_tz": -60
    },
    "id": "iblbzPuIH57q",
    "outputId": "e21e0483-0270-4d1a-fd83-afaf91555ab3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.45094822, 0.18249191])"
      ]
     },
     "execution_count": 104,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we want to reduce the dimensionality of our data to 2 dimensions for easy visualization\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# fit our pca object to the scaled data\n",
    "X_train_pca = pca.fit_transform(scale(X_train))\n",
    "\n",
    "# explained variance is the fraction of the total variance in the entire dataset that a principal component accounts for\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SXkX_J9fIzU3"
   },
   "outputs": [],
   "source": [
    "# visualize PC1 vs PC2 with color as the target\n",
    "plt.figure(figsize=(8,6))\n",
    "plot = plt.scatter(X_train_pca[:,0], X_train_pca[:,1], c=y_train,\n",
    "                   linewidths=1, edgecolors='white', cmap='RdYlGn')\n",
    "plt.legend(*plot.legend_elements(),\n",
    "                    loc=\"upper left\", title=\"Classes\")\n",
    "plt.xlabel('Principal component 1')\n",
    "plt.ylabel('Principal component 2')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 506,
     "status": "ok",
     "timestamp": 1619356790822,
     "user": {
      "displayName": "George Stronge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjB4k01tg6PbeY45zroI_SbiF8yAjZ2njmJtl691w=s64",
      "userId": "17668309495408135664"
     },
     "user_tz": -60
    },
    "id": "zv0QAI-0Jm5q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 936,
     "status": "ok",
     "timestamp": 1619356796060,
     "user": {
      "displayName": "George Stronge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjB4k01tg6PbeY45zroI_SbiF8yAjZ2njmJtl691w=s64",
      "userId": "17668309495408135664"
     },
     "user_tz": -60
    },
    "id": "VyQPp-z5JFKu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_e3WESeL0gR"
   },
   "source": [
    "Yes, it would seem that applying logistic regression on our reduced version of all 30 features achieves slightly better performance than simply selecting the two most powerful features! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fi_OktFTKJpX"
   },
   "outputs": [],
   "source": [
    "# obtain model parameters/coefficients\n",
    "b = clf2.intercept_[0]\n",
    "w1, w2 = clf2.coef_.T\n",
    "\n",
    "# solve for the intercept and gradient of the decision boundary\n",
    "c = -b/w2\n",
    "m = -w1/w2\n",
    "\n",
    "# visualise the decision boundary\n",
    "xmin, xmax = -10, 16\n",
    "ymin, ymax = -7, 10\n",
    "xd = np.array([xmin, xmax])\n",
    "yd = m*xd + c\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(xd, yd, 'k', lw=1, ls='--')\n",
    "plt.fill_between(xd, yd, ymin, color='tab:purple', alpha=0.2)\n",
    "plt.fill_between(xd, yd, ymax, color='tab:green', alpha=0.2)\n",
    "plot = plt.scatter(X_train_pca[:,0], X_train_pca[:,1], c=y_train,\n",
    "                   linewidths=1, edgecolors='white', cmap='RdYlGn')\n",
    "plt.legend(*plot.legend_elements(),\n",
    "                    loc=\"upper left\", title=\"Classes\")\n",
    "plt.xlabel('Principal component 1')\n",
    "plt.ylabel('Principal component 2')\n",
    "plt.ylim(-7,10)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "58VrxPq1yWh5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNDUD4398PNFc59L9e3KE+H",
   "collapsed_sections": [],
   "name": "Lecture1-Regression_KMeans_PCA_Practical.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
