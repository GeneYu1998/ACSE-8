{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "y_8RzRMMGzlp",
    "outputId": "0ac42ff7-73af-4f39-fe77-53e296aec9da"
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xfr5p6P3Gzlv"
   },
   "source": [
    "## Afternoon session 2:\n",
    "### Optimization and Neural Networks\n",
    "\n",
    "Read the following paper: [An overview of gradient descent optimization\n",
    "algorithms](https://arxiv.org/pdf/1609.04747.pdf).  \n",
    "(At least read about stochastic, full-batch and mini-batch gradient descent, SGD, Momentum and Adam.)\n",
    "\n",
    "Then try to implement the following:\n",
    "\n",
    "2. Compare different optimization methods using the Ackley-Exercise\n",
    "    - Create plots for SGD, SGD+Momentum=0.9, RMSProp, Adam\n",
    "    - Compare for a number of learning rates lr=[10, 1, 1e-1, 1e-2]\n",
    "    - The final result should be a 4x4 matrix of plots of the Ackley function optimized from 100 starting locations (set_seed(42)) to get the same locations every time.\n",
    "    - What do you observe in terms of local and global minima?\n",
    "3. For the final exercise from the morning session create a network that achieves at least 98% accuracy on the test set. You may add layers, add weights, change learning rates and optimization methods. Use full-batch training.\n",
    "\n",
    "Additional Reading Material:  \n",
    "[Visualizing the Loss Landscapes of Neural Networks](https://papers.nips.cc/paper/7875-visualizing-the-loss-landscape-of-neural-nets.pdf)  \n",
    "[On the importance of initialization and momentum in deep learning](http://proceedings.mlr.press/v28/sutskever13.pdf)  \n",
    "[Why Momentum really works distill.pub](https://distill.pub/2017/momentum/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qr4GDzSiGzl0"
   },
   "source": [
    "## Ackely Investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WCARs9qPGzl0"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "IM38oUtOGzl0"
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "\n",
    "def ackley(x, y):\n",
    "    sum_sq_term = -20 * np.exp(-0.2*np.sqrt(0.5*(x*x+y*y)))\n",
    "    cos_term = -np.exp(0.5*(np.cos(2*np.pi*x)+np.cos(2*np.pi*y)))\n",
    "    value = sum_sq_term+cos_term+np.exp(1)+20\n",
    "    return value\n",
    "\n",
    "class Ackley(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, coords):\n",
    "        x = coords[:, 0]\n",
    "        y = coords[:, 1]\n",
    "        sum_sq_term = -20 * torch.exp(-0.2*torch.sqrt(0.5*(x*x+y*y)))\n",
    "        cos_term = -torch.exp(0.5*(torch.cos(2*np.pi*x)+torch.cos(2*np.pi*y)))\n",
    "        value = sum_sq_term+cos_term+np.exp(1)+20\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "6bREz4n8Gzl1"
   },
   "outputs": [],
   "source": [
    "def optimize_ackley(method, lr, N_points=100):\n",
    "    set_seed(42)\n",
    "    ackley_torch = Ackley()\n",
    "\n",
    "    attempts = []\n",
    "    for j in range(N_points):\n",
    "        coords = torch.randn(1, 2)*2\n",
    "        coords.requires_grad = True\n",
    "        if method == \"SGD\":\n",
    "            # add SGD optimizer\n",
    "        elif method == \"SGD+Momentum\":\n",
    "            # add SGD + momentum\n",
    "        elif method == \"RMSProp\":\n",
    "            # add RMSProp\n",
    "        elif method == \"Adam\":\n",
    "            # add Adam  \n",
    "        steps = [coords.detach().numpy().copy()]\n",
    "        for i in range(50):\n",
    "            # reset grads with zero_grad\n",
    "            # instantiate object of ackley_torch class\n",
    "            # compute gradients\n",
    "            # step\n",
    "            steps.append(coords.detach().numpy().copy())\n",
    "        attempts.append(steps)\n",
    "    return attempts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "U7xYXUgAGzl1"
   },
   "outputs": [],
   "source": [
    "def plot_ackley_and_trajectories(ax, attempts, method, lr):\n",
    "    x = np.linspace(-3, 3, 100)\n",
    "    y = np.linspace(-3, 3, 100)\n",
    "    # Make data.\n",
    "    x, y = np.meshgrid(x, y)\n",
    "    z = ackley(x, y)\n",
    "\n",
    "    # Plot the surface.\n",
    "    surf = ax.contourf(x, y, z, np.linspace(0, 10, 100), cmap=cm.coolwarm)\n",
    "    for a in attempts:\n",
    "        steps_np = np.array(a)[:, 0, :]\n",
    "        ax.plot(steps_np[:, 0], steps_np[:, 1], linewidth=2, c=\"black\", linestyle=\"--\", alpha=0.1)\n",
    "\n",
    "    for a in attempts:\n",
    "        steps_np = np.array(a)[:, 0, :]\n",
    "        ax.scatter(steps_np[[0], 0], steps_np[[0], 1], marker=\"o\", color=\"yellow\", s=50, zorder=100)\n",
    "        ax.scatter(steps_np[[-1], 0], steps_np[[-1], 1], marker=\"o\", color=\"magenta\", s=50, zorder=100)\n",
    "    ax.set_xlim(-3, 3)\n",
    "    ax.set_ylim(-3, 3)\n",
    "    ax.set_title(method+\" \"+str(lr))\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "9NPWntTOGzl2",
    "outputId": "e671dc30-c8d0-4b2d-bb34-bc98e95d9b8b"
   },
   "outputs": [],
   "source": [
    "fig, axarr = plt.subplots(4, 4, figsize=(24, 24))\n",
    "# loop over methods \n",
    "    # loop over learning rates\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YpY158GVGzl3"
   },
   "source": [
    "Learning Rate 10: Trajectories diverge, no global minima found, no local minima found  \n",
    "Learning Rate 1: Most Trajectories diverge, only Adam finds global and some local minima  \n",
    "Learning Rate 1e-2: SGD finds points clode to global minimum, SGD+Momentum overshoots, Adam finds all local minima  \n",
    "Learning Rate 1e-3: All learning methods dive into local minima, SGD+momentum and Adam have longer trajectories and find global minimum more often.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBtqGo_xGzl3"
   },
   "source": [
    "## Improving the two-moons neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mIat9JaFGzl3",
    "outputId": "d63bf7e7-f1cc-4f4a-bb87-9ecd67762f9a"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Afternoon-Session-2-Diving-into-Optimization-and-Neural-Nets-Exercises_2_and_3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
