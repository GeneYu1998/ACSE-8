{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dCaiuZ2yDEzY",
    "outputId": "7fcd5426-f275-435b-ce78-3d951ee42840"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YhlkaEYCDEzi"
   },
   "source": [
    "## Afternoon session 2:\n",
    "### Optimization and Neural Networks\n",
    "\n",
    "Read the following paper: [An overview of gradient descent optimization\n",
    "algorithms](https://arxiv.org/pdf/1609.04747.pdf).  \n",
    "(At least read about stochastic, full-batch and mini-batch gradient descent, SGD, Momentum and Adam.)\n",
    "\n",
    "Then try to implement the following:\n",
    "1. Implement Momentum Accelerated gradient-descent for your backprop from scratch example.\n",
    "    - If you have not implemented your own version this morning, try to finish this or use the provided example code.\n",
    "    - The provided code also includes a bias term. \n",
    "    - All you have to implmement are the updates for the bias terms.\n",
    "    - Compare your results from the 1-hidden layer network with bias, with (momentum=0.9) and without momentum.\n",
    "    - Does this improve your results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zDU8mLK_DEzi"
   },
   "source": [
    "### Backprop from Scratch with Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nS0AjHMHDEzj"
   },
   "outputs": [],
   "source": [
    "def make_train_test(batch_size, batch_num, test_size, noise=0.05):\n",
    "    \"\"\"\n",
    "    Makes a two-moon train-test dataset with fixed batch size, number and noise level\n",
    "    \"\"\"\n",
    "    X_train, y_train = make_moons(n_samples=batch_size*batch_num, noise=noise)\n",
    "    y_train = y_train.reshape(batch_num, batch_size, 1)\n",
    "    X_train = X_train.reshape(batch_num, batch_size, 2)\n",
    "\n",
    "\n",
    "    X_test, y_test = make_moons(noise=0.1)\n",
    "    y_test = y_test.reshape(test_size, 1)\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"\n",
    "    Use this to set ALL the random seeds to a fixed value and take out any randomness from cuda kernels\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    torch.backends.cudnn.benchmark = False  ##uses the inbuilt cudnn auto-tuner to find the fastest convolution algorithms. -\n",
    "    torch.backends.cudnn.enabled   = False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nidblC578pea"
   },
   "source": [
    "Step 1: fill in the following values according to the morning session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "txrPIdc49fLk"
   },
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "\n",
    "epochs =  #Number of loops through whole dataset\n",
    "\n",
    "batch_size =  #Size of a single batch\n",
    "batch_num =  #Use full batch training\n",
    "test_size =  #Examples in test set\n",
    "\n",
    "lr = 1.\n",
    "D, H, M =  #Define input size (2), Size of Hidden Layer (4), Output size (1)\n",
    "momentum =  #Set to 0.9 to try momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jfwtUvgR9fx1"
   },
   "source": [
    "Step 2: create a two moon dataset with the parameters defined above and convert them to torch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m1Keqbte9yMo"
   },
   "outputs": [],
   "source": [
    "#Use Sklearn to create two-moons + noise\n",
    "\n",
    "#Define Train Set in Pytorch\n",
    "\n",
    "#Define Test Set in Pytorch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v_SizjmP9yvm"
   },
   "source": [
    "Step 3: define activation function and neural network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4f6eSnRS9_rA"
   },
   "outputs": [],
   "source": [
    "#Define Activation Functions and Derivatives\n",
    "\n",
    "#Define Neural Network Parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sKY_uXcu-TDr"
   },
   "source": [
    "Step 4: define and initialise momentum parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H5xSUMDh-p6M"
   },
   "outputs": [],
   "source": [
    "#Define the momentum parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SjRL7L3W-uYB"
   },
   "source": [
    "Step 5: create the training loop and print the loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ikrONep_E8M3"
   },
   "outputs": [],
   "source": [
    "#Enter training loop\n",
    "for i in range(epochs):\n",
    "    #Number of input examples\n",
    "    #Forward Pass Layer 1\n",
    "        #Affine Layer Transformation z1 = W1*X+b1\n",
    "        #Apply non-linear activation function a1 = sigmoid(z1)\n",
    "    \n",
    "    #Forward Pass Layer 2\n",
    "        #Affine Layer Transformation z2 = W2*a1+b2\n",
    "        #Apply non-linear activation function a2 = sigmoid(z2)\n",
    "\n",
    "    #Backward Pass Layer 2\n",
    "        #Compute Error on Output\n",
    "        #Compute derivative of activation function (Sigmoid)\n",
    "    \n",
    "    #Compute gradient w.r.t. weights in layer 2       \n",
    "    #Compute gradient w.r.t. bias in layer 2, sums over all N examples\n",
    "    \n",
    "    #Backward Pass Layer 1\n",
    "    #Compute Error on Output of Layer 1\n",
    "    #Compute derivative of activation function (Sigmoid)\n",
    "    \n",
    "    #Compute gradient w.r.t. weights in layer 2\n",
    "    #Compute gradient w.r.t. bias in layer 1, sums over all N examples\n",
    "\n",
    "    #Sensitivity w.r.t. Input\n",
    "    #Compute gradient w.r.t. input X\n",
    "    \n",
    "    #Gradient Descent with Momentum\n",
    "    if momentum is not None:\n",
    "        #Momentum step for layer 1 weights\n",
    "        #Take a step in momentum weighted direction on layer 1 weights\n",
    "        \n",
    "        #Momentum step for layer 2 weights\n",
    "        #Take a step in momentum weighted direction on layer 2 weights\n",
    "               \n",
    "        #Momentum step for layer 1 bias\n",
    "        #Take a step in momentum weighted direction on layer 1 bias     \n",
    "        \n",
    "        #Momentum step for layer 2 bias\n",
    "        #Take a step in momentum weighted direction on layer 2 bias     \n",
    "        \n",
    "    else: #Gradient Descent\n",
    "        #Take a step in gradient direction on layer 1 weights\n",
    "        #Take a step in gradient direction on layer 1 bias\n",
    "\n",
    "        #Take a step in gradient direction on layer 2 weights\n",
    "        #Take a step in gradient direction on layer 2 bias\n",
    "    \n",
    "    train_loss = -1./N*(y*torch.log(a2)+(1-y)*torch.log(1-a2)).sum(0) # Compute Average Binary-Crossentropy Loss\n",
    "    if i % 100 == 0:\n",
    "        print(\"Training Loss in epoch \"+str(i)+\": %1.2f\" % train_loss.item())\n",
    "        print(\"Training accuracy in epoch \"+str(i)+\": %1.2f\" % accuracy_score(y, np.where(a2[:, 0].numpy()>0.5, 1, 0)),\"\\n\")\n",
    "        \n",
    "#Do Forward Pass of Test Dataset\n",
    "#Forward Pass Layer 1\n",
    "    #Affine Layer Transformation z1 = W1*X+b1\n",
    "    #Apply non-linear activation function a1 = sigmoid(z1)\n",
    "\n",
    "#Forward Pass Layer 2\n",
    "    #Affine Layer Transformation z2 = W2*a1+b2\n",
    "    #Apply non-linear activation function a2 = sigmoid(z2)\n",
    "test_loss = -(y_test*torch.log(a_test)+(1-y_test)*torch.log(1-a_test)).mean(0) #Compute Binary-Crossentropy Loss\n",
    "\n",
    "print(\"End of Training -> Testing Phase: \")\n",
    "print(\"Train Loss: %1.2f\" % train_loss.item(), \", Test Loss: %1.2f\" % test_loss.item())\n",
    "print(\"Training accuracy in epoch \"+str(i)+\": %1.2f\" % accuracy_score(y, np.where(a2[:, 0].numpy()>0.5, 1, 0)))\n",
    "print(\"Test accuracy in epoch \"+str(i)+\": %1.2f\" % accuracy_score(y_test, np.where(a_test[:, 0].numpy()>0.5, 1, 0)))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NVndU4UaAGoB"
   },
   "source": [
    "Plot results (use the same plotting code from the morning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yeqGeFERFDqD"
   },
   "outputs": [],
   "source": [
    "# plot results"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Afternoon-Session-2-Diving-into-Optimization-and-Neural-Nets-Exercise_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
