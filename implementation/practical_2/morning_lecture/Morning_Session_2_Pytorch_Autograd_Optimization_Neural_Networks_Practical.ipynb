{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    },
    "colab": {
      "name": "Morning-Session-2-Pytorch-Autograd-Optimization-Neural-Networks-Practical.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C07-CTTAHbee"
      },
      "source": [
        "#### Notebook modified from the original version by `Lukas Mosser` and `Navjot Kukreja`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YcR72kmZ_39"
      },
      "source": [
        "%pylab inline\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def set_seed(seed):\n",
        "    \"\"\"\n",
        "    Use this to set ALL the random seeds to a fixed value and take out any randomness from cuda kernels\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    torch.backends.cudnn.benchmark = False  # uses the inbuilt cudnn auto-tuner to find the fastest convolution algorithms. Useful when inputs do not change size -\n",
        "    torch.backends.cudnn.enabled   = False\n",
        "\n",
        "    return True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJk5-kWy6YW5"
      },
      "source": [
        "You can check the [`backends.cudnn` documentation](https://pytorch.org/docs/stable/backends.html) for more info on how to tune your [cuDNN library](https://docs.nvidia.com/deeplearning/cudnn/index.html) (and other backends)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObSXVsEvZ_4D"
      },
      "source": [
        "!pip install torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecoQbj3VZ_4D"
      },
      "source": [
        "# ACSE Module 8 - Practical - Morning Session 2 Solutions:\n",
        "# Pytorch, Automatic Differentiation, Optimization\n",
        "\n",
        "## Objectives of the day:\n",
        "\n",
        "- Understand optimisation in the context of Machine Learning\n",
        "\n",
        "- Understand the mechanics of PyTorch to train a network (optimisation problem)\n",
        "\n",
        "<br>\n",
        "\n",
        "## Optimisation, training and backpropagation concepts in supervised learning\n",
        "<hr>\n",
        "<br>\n",
        "\n",
        "While it is possible to train an unsupervised network (as we will see when we visit autoencoders), we will focus today on supervised learning.\n",
        "\n",
        "<br>\n",
        "\n",
        "Assume we have a fully connected neural network designed to classify hand-written numbers from the MNIST dataset:\n",
        "\n",
        "<br>\n",
        "\n",
        "<img src=\"https://thumbs.gfycat.com/AdorableJoyfulLemming-max-1mb.gif\" alt=\"network\" width=\"800\"/>\n",
        "\n",
        "<br>\n",
        "\n",
        "The parameters $\\theta_{ij}^{(k)}$ of the network control how well the network assigns a digit to its correct class, and as you can see the network in the gif above already knows what it's doing as the softmax activation function in the last layer provides the largest probability to the correct class (7).\n",
        "\n",
        "But when we attack a new problem, we only have a dataset and the intention to use a neural network to solve the problem. We will not talk today about how to choose the best network architecture (that is, how many hidden layers, how many neurons per layer, what activation functions to use, etc, as this will be covered in future sesions).\n",
        "\n",
        "Let's assume for now that we have decided to use a particular network architecture. This network is untrained, and by that we mean that since it has never seen the data, we have no idea of what the network parameters should be. The common approach is then, assign random numbers to this parameters and see what happens.\n",
        "\n",
        "This happens:\n",
        "\n",
        "<br>\n",
        "\n",
        "<img src=\"https://thumbs.gfycat.com/ContentDarlingCub-size_restricted.gif\" alt=\"network\" width=\"800\"/>\n",
        "\n",
        "<br>\n",
        "\n",
        "So the question now is how do we improve these parameters so that the network does what we want (which is to correctly classify my digits in this case)? The answer is to train the network.\n",
        "\n",
        "Training means that we will use data for which we know the answer to modify the parameters $\\theta$ of our network by defining a loss function that captures the accuracy of the predictions of our network.\n",
        "\n",
        "Why do we use local optimisation techniques (based on gradient descent methods and variations) to train our networks? Remember how many parameters where there in the network from yesteraday's talk? \n",
        "\n",
        "The loss function defines a hypersurface in a multi-dimensional space that has as many dimensions as there are network parameters. This hypersurface is known as the [solution space, the feasible region, the search space or the feasible set](https://en.wikipedia.org/wiki/Feasible_region).\n",
        "\n",
        "So the goal of training is to find the lowest point in this solution space. In a very simple case with two dimensions (two network parameters) we will do this:\n",
        "\n",
        "<img src=\"https://camo.githubusercontent.com/8ea43a1f70612fe0be12346955ed5befd10c6bbdf2e850d04fe3bf865d25445d/68747470733a2f2f707669676965722e6769746875622e696f2f6d656469612f696d672f70617274312f6772616469656e745f64657363656e742e676966\" alt=\"optimisation\" width=\"800\"/>\n",
        "\n",
        "<br>\n",
        "\n",
        "And in order to achieve this we will use **backpropagation**.\n",
        "\n",
        "**Backpropagation** is a method that will let us improve our network parameters by iteratively updating them in a direction that reduces the magnitude of the loss.\n",
        "\n",
        "And now, it (hopefully) has become clearer why we use local optimisation: we can afford to compute the value of the loss for a particular set of $\\theta$ values, as well as the gradient of the loss with respect to these $\\theta$ values using backpropagation. But using a global method (like Monte Carlo optimisation) would be unaffordable because that would require to run forward passes of the network a ridiculously large number of times (remember we are in very large dimensional spaces). \n",
        "\n",
        "To avoid having to manually code a lot of the functionalities required to implement our networks, we will use PyTorch.\n",
        "\n",
        "## PyTorch\n",
        "<hr>\n",
        "\n",
        "[Pytorch](https://pytorch.org) is a (almost) drop-in replacement to numpy functionality but with added automatic differentiation capabilities.  \n",
        "\n",
        "## Awesome [Pytorch](https://www.pytorch.org) functionalities and where to find them:\n",
        "\n",
        "- [torch.Tensor](https://pytorch.org/docs/stable/tensors.html): Fundamental Tensor operations (matmul, sum, mean, transpose, ...)\n",
        "- [torch.nn](https://pytorch.org/docs/stable/nn.html): Specialised functions for implementing (deep) neural networks\n",
        "    - [torch.nn.DataParallel](https://pytorch.org/docs/stable/nn.html#dataparallel-layers-multi-gpu-distributed): Parallel and Distributed Computing\n",
        "    - [Activation Functions](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity): Sigmoid, Tanh, ReLU, ...\n",
        "    - [Linear Layers](https://pytorch.org/docs/stable/nn.html#linear-layers)\n",
        "    - [Convolutional Layers](https://pytorch.org/docs/stable/nn.html#convolution-layers)\n",
        "    - [Loss Functions](https://pytorch.org/docs/stable/nn.html#loss-functions): MSE-Loss, CrossEntropyLoss, ...\n",
        "- [torch.optim](https://pytorch.org/docs/stable/optim.html): First and Second-order Gradient Descent Optimizers\n",
        "- [torch.autograd](https://pytorch.org/docs/stable/autograd.html): Automatic Differentiation Functionality\n",
        "- [torch.distributions](https://pytorch.org/docs/stable/distributions.html): Probability Distributions\n",
        "- [torch.utils](): Utility functions\n",
        "    - [torch.utils.data](https://pytorch.org/docs/stable/data.html): Contains useful methods to load and handle data\n",
        "- [torchvision](https://pytorch.org/vision/stable/index.html): Datasets, Pre-trained Models, Transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syh_M3b93GLS"
      },
      "source": [
        "## Task 0: Basic Pytorch Tensor Operations \n",
        "\n",
        "### 0.1: setting a seed and declaring variables (tensors)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwQA1tzPZ_4E"
      },
      "source": [
        "#Setting the random seed of torch, numpy and python's random module\n",
        "set_seed(42) # so that we all get the same results (that's all it does)\n",
        "\n",
        "#Declare a scalar value\n",
        "a = torch.Tensor(1)\n",
        "print(a, a.item())\n",
        "\n",
        "#Declare a tensor like another tensor - creates tensor of the same shape\n",
        "b = torch.zeros_like(a)\n",
        "print(b, b.item())\n",
        "\n",
        "#Create a torch.Tensor from a numpy array - doesn't copy memory! :)\n",
        "c = torch.from_numpy(np.array(range(42))) \n",
        "print(c)\n",
        "\n",
        "#Get back the underlying numpy array\n",
        "print(c.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hurTwSsZ_4G"
      },
      "source": [
        "### 0.2: Create a random array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9DNge9tZ_4Q"
      },
      "source": [
        "#Create a tensor of Gaussian (0-mean, 1-std. dev.) values\n",
        "m = torch.randn(1, 1, 28, 28)\n",
        "print(m.size())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4v3jEsWZ_4R"
      },
      "source": [
        "### 0.3: Plotting Pytorch Tensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZZjq-HOZ_4R"
      },
      "source": [
        "plt.imshow(m[0, 0, :, :]) # equal to plt.imshow(m[0,0])\n",
        "plt.colorbar()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYTEykpVZ_4S"
      },
      "source": [
        "### 0.3: Using a GPU\n",
        "We can also make use of a GPU (if we have one) by specifying ```.cuda()``` on any Tensors and later Modules.\n",
        "\n",
        "[torch.cuda](https://pytorch.org/docs/stable/cuda.html) package documentation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayBhT-KoZ_4S"
      },
      "source": [
        "device = 'cpu'\n",
        "if torch.cuda.device_count() > 0 and torch.cuda.is_available():\n",
        "    print(\"Cuda installed! Running on GPU!\")\n",
        "    device = 'cuda'\n",
        "else:\n",
        "    print(\"No GPU available!\")\n",
        "    \n",
        "#Move tensors around using the .to(device) command\n",
        "tensor_on_device = torch.ones(1).to(device)\n",
        "print(tensor_on_device.device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPXwnbRhF1A8"
      },
      "source": [
        "`cuda:0` means that the tensor we have moved to the device is on the device `cuda:0` (which is the GPU we are using)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEGgvMh_Z_4S"
      },
      "source": [
        "### 0.4: Pytorch Autograd with simple functions\n",
        "\n",
        "Pytorch allows us to write pythonic functions and use them in our computational graph."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aAMl8JoZ_4U"
      },
      "source": [
        "a = torch.ones(1, requires_grad=True)*np.pi\n",
        "print(a)\n",
        "def f(x):\n",
        "    return torch.sin(x)\n",
        "y = f(a)\n",
        "print(a.item(), y.item())\n",
        "\n",
        "# We can explicitly calculate gradients of a function with respect to a parameter or set of parameters:\n",
        "print(\"The derivative is: \", torch.autograd.grad(y, a)[0].item()) # [0] because .grad returns a tuple and we need to extract the value (it only has one element)\n",
        "# print(\"The derivative is: \", torch.autograd.grad(y, a)[0].item()) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZB3ksDVZ_4U"
      },
      "source": [
        "### 0.5: **Pytorch Modules** (this is important as you will use `nn.Module` often)\n",
        "\n",
        "We will make a simple parametric parabola as a Pytorch module.  \n",
        "We don't need to write a backward (derivative) for pytorch modules as they are "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xj0cdzsiZ_4V"
      },
      "source": [
        "class ParametricParabola(nn.Module):\n",
        "    def __init__(self, a, b):\n",
        "        super().__init__()\n",
        "        self.a = torch.nn.Parameter(torch.ones(1, requires_grad=True)*a)\n",
        "        self.b = torch.nn.Parameter(torch.ones(1, requires_grad=True)*b)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.a*x*x+self.b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqDJxYp7H75Z"
      },
      "source": [
        "[a good stackoverflow explanation on supercharing classes](https://stackoverflow.com/questions/576169/understanding-python-super-with-init-methods) (if you are curious to know how it works in detail, and **completely optional**)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIkk_uB5Z_4V"
      },
      "source": [
        "Let's inspect what the parameters are:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4PWkeWAZ_4V"
      },
      "source": [
        "f = ParametricParabola(0.5, 2)\n",
        "print(f.a)\n",
        "print(f.b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uw6MmN7HZ_4W"
      },
      "source": [
        "We can also compute gradients for this example with respect to the parameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOKydd5WZ_4W"
      },
      "source": [
        "f.zero_grad() #this resets all the gradients within the computational graph.\n",
        "x = torch.ones(1, requires_grad=False)*np.pi # we don't need the grad with respect to x now, we will look at the grad with respect to the parameters of the parabola.\n",
        "y = f(x) # you need to call it like this to make sure that torch takes care of the registered hooks. Calling it as a method does not do that.*\n",
        "y.backward() #call autograd to compute all partial derivatives with respect to all Parameters that require gradients\n",
        "print(f.a.grad, f.b.grad)\n",
        "print(f.a.grad.item(),f.b.grad.item()) # use .item() to print the value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjvN-5gIOWLR"
      },
      "source": [
        "What do you think the values printed are?\n",
        "\n",
        "\\*[more about hooks here](https://blog.paperspace.com/pytorch-hooks-gradient-clipping-debugging/) (if you are interested, and again **absolutely optional**)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8JcECg5Z_4W"
      },
      "source": [
        "### 0.6 Using Pytorch's Optimization functionality\n",
        "\n",
        "We can use methods from ```torch.optim``` to optimize parameters.  \n",
        "We use this here to find the global minimum of our parabola with gradient descent.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrvuPDcVZ_4W"
      },
      "source": [
        "set_seed(42)\n",
        "x = torch.from_numpy(np.array([50])).float()\n",
        "x.requires_grad = True  # now we do need the grad with respect to x, as we are optimising for it.\n",
        "f = ParametricParabola(2., 0.)\n",
        "optimizer = torch.optim.SGD([x], lr=1e-2) # also, play with the learning rate to see how it affects the results\n",
        "for i in range(1000):\n",
        "    optimizer.zero_grad()\n",
        "    value = f(x)\n",
        "    value.backward()\n",
        "    optimizer.step()\n",
        "    if i % 100 == 99 or i == 0 or i == 1 or i == 10 or i == 50:\n",
        "        print(\"Iteration \", i, \" Functional value: \", value.item(), '      x= ', x.item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMMCzFRijftR"
      },
      "source": [
        "### What did we just optimised for here? Network parameters?\n",
        "\n",
        "<br>\n",
        "\n",
        "####NOTE: difference between `.grad` and `.backward`:\n",
        "\n",
        "The difference is that `.grad()` is returning the gradients to you.\n",
        "While `.backward()` is populating the `.grad` field on the different leaf Tensors that were used to compute y.\n",
        "\n",
        "In particular, this `.grad` field is used by the optimizers to update the weights. So if you use `.grad()` you will need to populate these fields yourself based on the gradients it returned before calling optimizer.step().\n",
        "\n",
        "[reference](https://discuss.pytorch.org/t/whats-the-difference-between-torch-autograd-grad-and-backward/94638)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bnm3td9_Z_4X"
      },
      "source": [
        "## Task 1: Plotting the Ackley Function using Numpy\n",
        "\n",
        "The [Ackley](https://en.wikipedia.org/wiki/Ackley_function) function is a common test problem for optimization problems with many local minima:\n",
        "\\begin{equation*}\n",
        "f(x, y)= -20 exp [-0.2\\sqrt{0.5(x^2+y^2)}]-exp[0.5\\{cos(2 \\pi x) + cos(2 \\pi y)\\}]+ e +20\n",
        "\\end{equation*}\n",
        "\n",
        "In the following problem tasks we will use pytorch to evaluate this function numerically and study its behavior under common gradient descent techniques."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfZIZPjbZ_4X"
      },
      "source": [
        "- 1.1: Define the Ackley-Function as a python function\n",
        "- 1.2: Use your previously defined function to plot the function in 3d in the range $(x, y)\\in [-5, 5]$\n",
        "- 1.3: Use your previously defined function Plot the function in 3d in the range $(x, y) \\in [-32, 32]$\n",
        "- 1.4: What do you observe about this function? Why do you think this function is a test problem for optimization?\n",
        "\n",
        "### 1.1: Define the Ackley-Function as a python method "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZ0E0_LpZ_4X"
      },
      "source": [
        "%matplotlib inline\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "import numpy as np\n",
        "\n",
        "def ackley(x, y):\n",
        "    # code the Ackley function\n",
        "    return value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNtvzu15Z_4X"
      },
      "source": [
        "### 1.2: Use your previous defined method to plot the function in 3d in the range $(x, y)\\in [-5, 5]$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppxqedU4Z_4X"
      },
      "source": [
        "def plot_ackley_3d(x, y):\n",
        "    fig = plt.figure()\n",
        "    ax = fig.gca(projection='3d')\n",
        "\n",
        "    # Make data.\n",
        "    x, y = np.meshgrid(x, y)\n",
        "    \n",
        "    z = ackley(x, y)\n",
        "\n",
        "    # Plot the surface.\n",
        "    surf = ax.plot_surface(x, y, z, cmap=cm.cividis,\n",
        "                                   linewidth=0, antialiased=False)\n",
        "\n",
        "    # Add a color bar which maps values to colors.\n",
        "    fig.colorbar(surf, shrink=0.5, aspect=5)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "x = np.linspace(-5, 5, 100)\n",
        "y = np.linspace(-5, 5, 100)\n",
        "plot_ackley_3d(x, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMZz0Z8IZ_4Y"
      },
      "source": [
        "### 1.3: Use your previous defined method Plot the function in 3d in the range $(x, y) \\in [-32, 32]$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2dbxrCHpdG0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EseFshZtjEg"
      },
      "source": [
        "### 1.3: Use your previous defined method Plot the function in 3d in the range $(x, y) \\in [-2, 2]$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3l5hov4tcrp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80MnTtNPZ_4Y"
      },
      "source": [
        "### 1.4: What do you observe about this function? Why do you think this function is a test problem for optimization?\n",
        "\n",
        "Plot it again as a contour to highlight its non-convex nature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6ih2NwCrUoH"
      },
      "source": [
        "x = np.linspace(-2, 2, 100)\n",
        "y = np.linspace(-2, 2, 100)\n",
        "X, Y = np.meshgrid(x, y)\n",
        "Z = ackley(X,Y)\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.contour(X,Y,Z)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJ6gEr-VZ_4Y"
      },
      "source": [
        "## Task 2: The Ackley Function in Pytorch, Autograd and Gradient Descent\n",
        "\n",
        "- 2.1: Define the Ackley-Function as a pytorch nn.Module class\n",
        "- 2.2: Try to find the global minimum (solution: 0., 0.) of the Ackley function with Gradient Descent (```torch.optim.SGD```) from a random starting position. Use torch.randn to obtain a sample from a Gaussian.\n",
        "    - Store and plot the corresponding function value as a function of the iteration number (ackley(x) vs. iteration)\n",
        "    - Store and plot the norm of the gradients during optimization.\n",
        "- 2.3: For 10 random starting positions (```torch.randn```) plot the Ackley function in 2D and the optimization trajectories together:\n",
        "    - Do you reach a global minimum?\n",
        "    - What do you observe for the various optimization trajectories.\n",
        "    \n",
        "### 2.1: Define the Ackley-Function as a pytorch nn.Module class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2aaq6dIZ_4Y"
      },
      "source": [
        "class Ackley(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    \n",
        "    def forward(self, coords):\n",
        "        x = coords[:, 0] # they are arrays that can be computed in groups\n",
        "        y = coords[:, 1] # instead of one at a time as we defined before\n",
        "        sum_sq_term = -20 * torch.exp(-0.2*torch.sqrt(0.5*(x*x+y*y))) # = -20 * np.exp(-0.2*np.sqrt(0.5*(x*x+y*y))) \n",
        "        cos_term = -torch.exp(0.5*(torch.cos(2*np.pi*x)+torch.cos(2*np.pi*y))) # -np.exp(0.5*(np.cos(2*np.pi*x)+np.cos(2*np.pi*y)))\n",
        "        value = sum_sq_term+cos_term+np.exp(1)+20 # no need to keep track with torch here, np is fine (no parameter in the call)\n",
        "        return value\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOPKf4BwZ_4Z"
      },
      "source": [
        "### 2.2: Find the global minimum (solution: 0., 0.) of the Ackley function with Gradient Descent \n",
        "\n",
        "and plot the functional value and the gradient norm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmpFhhi16MBZ"
      },
      "source": [
        "set_seed(42)\n",
        "#set_seed(621)   \n",
        "ackley_torch = Ackley()\n",
        "\n",
        "coords = torch.randn(1, 2, requires_grad=True) # only one pair\n",
        "optimizer = torch.optim.Adam([coords], lr=1e-2, betas=(0.5, 0.9)) # new optimiser introduced here\n",
        "steps = [] # initialise steps and grads as\n",
        "grads = [] # empty lists\n",
        "for i in range(50):\n",
        "    optimizer.zero_grad() # remember to reset the gradient information\n",
        "    f = ackley_torch(coords)\n",
        "    f.backward()\n",
        "    optimizer.step() \n",
        "    steps.append(f.detach().numpy().copy()) # keep track of the functional value\n",
        "    grads.append(coords.grad.norm().detach().numpy().copy()) # keep track of the gradients\n",
        "    if i % 10 == 9 or i == 0:\n",
        "      print('current X = {0:5f}, current Y = {0:5f}'.format(coords[0,0].item(), coords[0,1].item()))\n",
        "\n",
        "print('')    \n",
        "fig, ax = plt.subplots(1,2, figsize=(12, 6))\n",
        "ax[0].plot(steps)\n",
        "ax[0].set_xlabel(\"Iteration Number\")\n",
        "ax[0].set_ylabel(\"Functional Value\")\n",
        "ax[1].plot(grads)\n",
        "ax[1].set_xlabel(\"Iteration Number\")\n",
        "ax[1].set_ylabel(\"Gradient Norm\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxqGID5sx6_l"
      },
      "source": [
        "### Again, what have we just optimised?\n",
        "\n",
        "<br>\n",
        "\n",
        "Documentation for the [Adam optimiser](https://pytorch.org/docs/stable/optim.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDQFYIwdZ_4Z"
      },
      "source": [
        "### 2.3: For 10 random starting positions (```torch.randn```) plot the Ackley function in 2D and the optimization trajectories together.\n",
        "\n",
        "Plot the trajectories on top of the contour plot, but use [contourf](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.contourf.html) this time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96gIoo7-6SK8"
      },
      "source": [
        "# set_seed\n",
        "# create an instance of the Ackley class \n",
        "\n",
        "# loop over different starting points and run the optimisation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feDrhTZC6TZr"
      },
      "source": [
        "x = np.linspace(-3, 3, 100)\n",
        "y = np.linspace(-3, 3, 100)\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(12, 12))\n",
        "\n",
        "# Make data.\n",
        "x, y = np.meshgrid(x, y)\n",
        "\n",
        "z = ackley(x, y)\n",
        "\n",
        "# Plot the surface.\n",
        "surf = ax.contourf(x, y, z, np.linspace(0, 10, 100), cmap=cm.gray_r)\n",
        "for a in attempts:\n",
        "    steps_np = np.array(a)[:, 0, :]\n",
        "    ax.plot(steps_np[:, 0], steps_np[:, 1], linewidth=2, c=\"black\", linestyle=\"--\", alpha=0.2)\n",
        "    \n",
        "for a in attempts:\n",
        "    steps_np = np.array(a)[:, 0, :]\n",
        "    ax.scatter(steps_np[[0], 0], steps_np[[0], 1], marker=\"o\", color=\"yellow\", s=100, zorder=100)\n",
        "    ax.scatter(steps_np[[-1], 0], steps_np[[-1], 1], marker=\"o\", color=\"deepskyblue\", s=100, zorder=100)\n",
        "\n",
        "ax.set_xlim(-3, 3)\n",
        "ax.set_ylim(-3, 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhd-XghmZ_4b"
      },
      "source": [
        "<br>\n",
        "\n",
        "----\n",
        "\n",
        "<br>\n",
        "\n",
        "## Definition of the half-moon dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWnVPE-AZ_4b"
      },
      "source": [
        "import torch\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "def make_train_test(batch_size, batch_num, test_size, noise=0.05):\n",
        "    \"\"\"\n",
        "    Makes a two-moon train-test dataset with fixed batch size, number and noise level\n",
        "    \"\"\"\n",
        "    X_train, y_train = make_moons(n_samples=batch_size*batch_num, noise=noise)\n",
        "    y_train = y_train.reshape(batch_num, batch_size, 1)\n",
        "    X_train = X_train.reshape(batch_num, batch_size, 2)\n",
        "\n",
        "\n",
        "    X_test, y_test = make_moons(n_samples = test_size, noise=0.1)\n",
        "    y_test = y_test.reshape(test_size, 1)\n",
        "    return X_train, y_train, X_test, y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XknU415pFMGV"
      },
      "source": [
        "make_moons [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39aFIKfHZ_4b"
      },
      "source": [
        "## Task 3: Backprop-From-Scratch using Numpy\n",
        "\n",
        "In the following exercise you will implement a simple 1-Hidden Layer Neural Network from scratch using ```torch```.\n",
        "We will use a simple toy dataset called the half-moon dataset as a training set.\n",
        "To perform this task we will outline the necessary steps here and provide you with pseudo-code for your implementation.\n",
        "\n",
        "1. Set the random number generator\n",
        "2. Which hyper-parameters will you need to set prior to training?\n",
        "3. Define the size of the input layer (D), the number of hidden layer units (H) and the output layer units (M).\n",
        "    - Suggestion: use a small number of neurons in the hidden layer e.g. H=3\n",
        "4. Define the training and test sets of your half-moon dataset.\n",
        "5. We will use ```sigmoid``` activation functions. Define functions to compute the forward and \"backward\"-pass of the sigmoid. Your function should take in a ```torch.Tensor``` and return a ```torch.Tensor```\n",
        "6. Define the weight tensors of each-layer. Initiallize the Weight-Tensors as ```torch.randn```. You should have two weight tensors W1, W2.\n",
        "7. Within a training loop perform the following operations for the forward pass\n",
        "    - Compute the affine layer transformation $z_1=W_1X$\n",
        "    - Compute the non-linear activation $a_1=\\sigma(z_1)$\n",
        "    - Compute the affine layer transformation $z_2=W_2a_1$\n",
        "    - Compute the non-linear activation $a_2=\\sigma(z_2)$\n",
        "    - Recall the chain-rule:\n",
        "    - Use the notes at the bottom to simplify the code\\*\n",
        "    - Compute the gradient of the Loss with respect to the weights of the output layer $\\frac{\\partial{L}}{\\partial{W_2}}=a_1^T*\\frac{\\partial{L}}{\\partial{a_2}}\\frac{\\partial{a_2}}{\\partial{z_2}}$. You will need to use ```torch.transpose``` and ```torch.matmul``` to perform this operation.\n",
        "    - Compute the error on the output of the hidden-layer $\\frac{\\partial{L}}{\\partial{a_1}}$\n",
        "    - Compute the gradient of the loss with respect to the hidden-layer weights $W_1$. This is the same operation as for the output layer.\n",
        "    - Bonus: Compute the sensitivity of the loss with respect to the input $\\frac{\\partial{L}}{\\partial{X}}$\n",
        "    - Perform a gradient descent step on the weights: $W_2^{t+1} = W_2^{t}-\\frac{\\alpha}{N}\\frac{\\partial{L}}{\\partial{W_2}}$. (Hint: the division by $N$ is necessary due to the ```torch.matmul``` operation being an effective summation over all the input examples.\n",
        "    - Compute the training loss as the binary cross entropy $BCE(y, a_2)=\\frac{1}{N}\\sum{y\\cdot log(a_2)+(1-y)\\cdot log(1-a_2)}$\n",
        "8. Perform the above iteration over a number of epochs (full-passes through the training set and use full-batch learning)\n",
        "8. After training, evaluate the performance on the test set by evaluating $y_{pred}=\\sigma(W_2\\sigma(W_1 X))$ and computing the accuracy using ```sklearn.metrics.accuracy_score```.\n",
        "9. Plot the prediction on the training and the test set.\n",
        "10. Bonus: Plot the sensitivity of the loss with respect to each datapoint in the input of the training set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJ5_523baN3U"
      },
      "source": [
        "### \\* Use the following to simplify your code:\n",
        "\n",
        "Derivative of `L` with respect to `a2`\n",
        "\n",
        "$L = - (y log(a_2) + (1-y) log(1-a_2))$\n",
        "\n",
        "$\\frac{\\partial L}{\\partial a_2} = \\frac{1-y}{1-a_2} - \\frac{y}{a_2}$\n",
        "\n",
        "$a_2 = \\sigma(z_2)$\n",
        "\n",
        "$\\frac{\\partial a_2}{\\partial z_2} = a_2 (1 - a_2) $\n",
        "\n",
        "$\\frac{\\partial L}{\\partial z_2} = \\frac{\\partial L}{\\partial a_2}  \\frac{\\partial a_2}{\\partial z_2} = (\\frac{1-y}{1-a_2} - \\frac{y}{a_2})  a_2  (1 - a_2) = (1-y)a_2 - y(1-a_2) = a_2 - a_2y - y + a_2y = a_2 - y$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6i9F003mZ_4b"
      },
      "source": [
        "set_seed(42)\n",
        "\n",
        "epochs = 1000 #Number of loops through whole dataset\n",
        "\n",
        "batch_size = 1000 #Size of a single batch\n",
        "batch_num = 1 #Use full batch training\n",
        "test_size = 100 #Examples in test set\n",
        "\n",
        "lr = 1.\n",
        "I, H, O = 2, 3, 1 #Define input size (2), Size of Hidden Layer (4), Output size (1)\n",
        "\n",
        "#Use Sklearn to create two-moons + noise\n",
        "X_train, y_train, X_test, y_test = make_train_test(batch_size, batch_num, test_size, noise=0.2)\n",
        "\n",
        "#Define Train Set in Pytorch\n",
        "X = torch.from_numpy(X_train).float()[0] #Convert to torch tensor, single batch\n",
        "y = torch.from_numpy(y_train).float()[0] #Convert to torch tensor, single batch\n",
        "\n",
        "#Define Test Set in Pytorch\n",
        "X_test = torch.from_numpy(X_test).float() #Convert to torch tensor, already single batch\n",
        "y_test = torch.from_numpy(y_test).float() #Convert to torch tensor, already single batch\n",
        "\n",
        "#Define Activation Functions and Derivatives\n",
        "#Sigmoid Activation Function\n",
        "#Derivative of Sigmoid Activation Function\n",
        "\n",
        "#Define Neural Network Parameters\n",
        "#Define the weight matrices\n",
        "\n",
        "#Enter training loop\n",
        "for i in range(epochs):\n",
        "    N = X.size(0) #Number of input examples\n",
        "    #Forward Pass Layer 1\n",
        "    #Affine Layer Transformation z1 = W1*X+b1\n",
        "    #Apply non-linear activation function a1 = sigmoid(z1)\n",
        "    \n",
        "    #Forward Pass Layer 2\n",
        "    #Affine Layer Transformation z2 = W2*a1+b2\n",
        "    #Apply non-linear activation function a2 = sigmoid(z2)\n",
        "\n",
        "    #Backward Pass Layer 2\n",
        "    #Compute Error on Output\n",
        "    #Compute derivative of activation function (Sigmoid)\n",
        "    \n",
        "    #Compute gradient w.r.t. weights in layer 2       \n",
        "    \n",
        "    #Backward Pass Layer 1\n",
        "    #Compute Error on Output of Layer 1\n",
        "    #Compute derivative of activation function (Sigmoid)\n",
        "    \n",
        "    #Compute gradient w.r.t. weights in layer 2\n",
        "\n",
        "    #Sensitivity w.r.t. Input\n",
        "    #Compute gradient w.r.t. input X\n",
        "\n",
        "    #Take a step in gradient direction on layer 1 weights\n",
        "\n",
        "    #Take a step in gradient direction on layer 2 weights\n",
        " \n",
        "    #Compute Average Binary-Crossentropy Loss\n",
        "    # if i % 100 == 0:\n",
        "    #     print(\"Training Loss in epoch \"+str(i)+\": %1.2f\" % train_loss.item())\n",
        "    #     print(\"Training accuracy in epoch \"+str(i)+\": %1.2f\" % accuracy_score(y, np.where(a2[:, 0].numpy()>0.5, 1, 0)),\"\\n\")\n",
        "        \n",
        "#Do Forward Pass of Test Dataset\n",
        "#Forward Pass Layer 1\n",
        "#Affine Layer Transformation z1 = W1*X\n",
        "#Apply non-linear activation function a1 = sigmoid(z1)\n",
        "\n",
        "#Forward Pass Layer 2\n",
        "#Affine Layer Transformation z2 = W2*a1\n",
        "#Apply non-linear activation function a2 = sigmoid(z2)\n",
        "#Compute Binary-Crossentropy Loss on the Test Set\n",
        "\n",
        "# print(\"End of Training -> Testing Phase: \")\n",
        "# print(\"Train Loss: %1.2f\" % train_loss.item(), \", Test Loss: %1.2f\" % test_loss.item())\n",
        "# print(\"Training accuracy in epoch \"+str(i)+\": %1.2f\" % accuracy_score(y, np.where(a2[:, 0].numpy()>0.5, 1, 0)))\n",
        "# print(\"Test accuracy in epoch \"+str(i)+\": %1.2f\" % accuracy_score(y_test, np.where(a_test[:, 0].numpy()>0.5, 1, 0)))      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhVqCFn9BqpT"
      },
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
        "ax[0].scatter(X[:, 0], X[:, 1], c=np.where(a2[:, 0].numpy()>0.5, 1, 0))\n",
        "ax[1].scatter(X_test[:, 0], X_test[:, 1], c=np.where(a_test[:, 0].numpy()>0.5, 1, 0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOcs6PQGBsUu"
      },
      "source": [
        "fig, ax = plt.subplots(1, 1, figsize=(7, 6))\n",
        "im = ax.scatter(X[:, 0], X[:, 1], c=dL_dX[:, 0].abs())\n",
        "fig.colorbar(im, ax=ax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIjfX_ozULV3"
      },
      "source": [
        "what do you think this means?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aTx69wSZ_4b"
      },
      "source": [
        "## Task 4: Training a Neural Network with Pytorch\n",
        "\n",
        "We will now perform the exact same exercise but using Pytorch's autograd functionality. \n",
        "Just as in the first few exercises we can create modules that contain all our \"layers\" and \"activations\".  \n",
        "Pytorch then allows us to automatically compute derivatives from the defined computational graph.  \n",
        "Pytorch essentially remembers all the operations that were performed on a dataset, and if you set ```requires_grad=True``` it will compute a gradient with respect to that parameter once you call backward on it.  \n",
        "Let's see how this works for our 1-hidden layer neural network.\n",
        "\n",
        "Here is the general workflow:\n",
        "\n",
        "- 5.1: Define a SingleHiddenLayer Network as a pytorch module\n",
        "- 5.2: Define the cost function\n",
        "- 5.3: Setup the training function\n",
        "- 5.4: Setup a validation/testing function\n",
        "- 5.5: Create a training/validation/testing split of your data\n",
        "- 5.6: Iterate over your dataset (epoch) and train your network using the train() and validate() methods\n",
        "- 5.7: Make Predictions on the training and test set and plot the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NiVAODhZ_4b"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKxXP_viZ_4c"
      },
      "source": [
        "### 4.1: Define a SingleHiddenLayer Network as a pytorch module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hldAlerpZ_4c"
      },
      "source": [
        "class SingleHiddenLayerNetwork(nn.Module):\n",
        "    def __init__(self, I, H, O):\n",
        "        super(SingleHiddenLayerNetwork, self).__init__()\n",
        "        # define hidden layer using nn.Linear\n",
        "        # define output layer using nn.Linear\n",
        "        # define the activation function\n",
        "        \n",
        "    def forward(self, X):\n",
        "        # compute z1\n",
        "        # compute a1\n",
        "        # compute z2\n",
        "        # compute a2\n",
        "        return a2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsQUMuSrZ_4c"
      },
      "source": [
        "### 4.2: Define the cost function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWL12XRPZ_4c"
      },
      "source": [
        "def bce_loss(y, a2):\n",
        "    return -1/y.size(0)*(y*a2.log()+(1-y)*(1-a2).log()).sum(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAGMyLyIZ_4c"
      },
      "source": [
        "### 4.3: Setup the training function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDrdlSRMZ_4c"
      },
      "source": [
        "def train(model, optimizer, data_loader):\n",
        "    model.train()\n",
        "    for X, y in data_loader:\n",
        "        # reset gradients\n",
        "        # forward propagtion through the model\n",
        "        # compute loss\n",
        "        # backpropagate\n",
        "        # update model parameters using the gradients\n",
        "    \n",
        "    y_pred = np.where(a2[:, 0].detach().numpy()>0.5, 1, 0)\n",
        "    accuracy = accuracy_score(y, y_pred)\n",
        "    return loss, accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gwnjHzcOEGR"
      },
      "source": [
        "DataLoader class [documentation](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTr4hdmvZ_4d"
      },
      "source": [
        "### 4.4: Setup a validation/testing function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_5RxExhZ_4d"
      },
      "source": [
        "def evaluate(model, data_loader):  \n",
        "    model.eval()\n",
        "    for X, y in data_loader:\n",
        "        with torch.no_grad():\n",
        "            a2 = model(X)\n",
        "            loss = bce_loss(y, a2)\n",
        "    y_pred = np.where(a2[:, 0].numpy()>0.5, 1, 0)\n",
        "    accuracy = accuracy_score(y, y_pred)\n",
        "    return loss, accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_d6B7WvhZ_4e"
      },
      "source": [
        "### 4.5: Create a training/validation/testing split of your data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeEKk74wZ_4e"
      },
      "source": [
        "#Define the size of the input, hidden, and output layers\n",
        "I, H, O = 2, 3, 1\n",
        "\n",
        "#Use Sklearn to create two-moons + noise\n",
        "X_train, y_train, X_test, y_test = make_train_test(batch_size, batch_num, test_size, noise=0.2)\n",
        "\n",
        "#Define Train Set in Pytorch\n",
        "X_train = torch.from_numpy(X_train).float()[0] #Convert to torch tensor, single batch\n",
        "y_train = torch.from_numpy(y_train).float()[0] #Convert to torch tensor, single batch\n",
        "\n",
        "train_dataset = TensorDataset(X_train, y_train) # wrapper around dataset that helps DataLoader\n",
        "\n",
        "#Define Test Set in Pytorch\n",
        "X_test = torch.from_numpy(X_test).float() #Convert to torch tensor, already single batch\n",
        "y_test = torch.from_numpy(y_test).float() #Convert to torch tensor, already single batch\n",
        "\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "#Use Pytorch's functionality to load data in batches. Here we use full-batch training again.\n",
        "train_loader = DataLoader(train_dataset, batch_size=X_train.size(0), shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=X_test.size(0), shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifVnon6KToSz"
      },
      "source": [
        "*NOTE:* TensorDataset in combination with DataLoader helps manage big datasets so that they do not have to be loaded in memory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRY6l6JoZ_4e"
      },
      "source": [
        "### 4.6: Iterate over your dataset (epoch) and train your network using the train() and validate() methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5pTGvTHZ_4e"
      },
      "source": [
        "network = SingleHiddenLayerNetwork(I, H, O)\n",
        "optim = torch.optim.SGD(network.parameters(), lr=1) # we can pass network.parameters to the optimiser\n",
        "                                                    # instead of passing an explicit list (useful for big networks)\n",
        "for i in range(1000):\n",
        "    # compute train loss and accuracy using the train function defined above\n",
        "    # compute test loss and accuracy using the evaluate function defined above\n",
        "    \n",
        "    if i % 100 == 0:\n",
        "        print(\"Training Loss in epoch \"+str(i)+\": %1.2f\" % train_loss.item())\n",
        "        print(\"Training accuracy in epoch \"+str(i)+\": %1.2f\" % train_accuracy)\n",
        "        print(\"Test Loss in epoch \"+str(i)+\": %1.2f\" % test_loss.item())\n",
        "        print(\"Test accuracy in epoch \"+str(i)+\": %1.2f\" % test_accuracy, \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s389Q3NLUoGF"
      },
      "source": [
        "After 200 epochs we have converged.\n",
        "\n",
        "Also note that the loss does not reach a zero value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o68ionRJZ_4f"
      },
      "source": [
        "### 4.7: Make Predictions on the training and test set and plot the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uH2eBxCHZ_4f"
      },
      "source": [
        "network.eval()   #   tell the network we are in evaluation mode (deactivates mini-batches, dropouts, etc)\n",
        "with torch.no_grad():    # deactivates the autograd engine (to not safe grads, etc)\n",
        "    a_train = network(X_train)\n",
        "    a_test = network(X_test)\n",
        "print(\"Test set accuracy: \", accuracy_score(y_test, np.where(a_test[:, 0].numpy()>0.5, 1, 0)))\n",
        "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
        "ax[0].scatter(X_train[:, 0], X_train[:, 1], c=np.where(a_train[:, 0].numpy()>0.5, 1, 0))\n",
        "ax[1].scatter(X_test[:, 0], X_test[:, 1], c=np.where(a_test[:, 0].numpy()>0.5, 1, 0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yh7RqCdlZ_4f"
      },
      "source": [
        "### Looks almost the same as our own backprop results!\n",
        "\n",
        "[`.eval` documentation](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.eval)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bT5uca-x9Ner"
      },
      "source": [
        "<br>\n",
        "\n",
        "-----\n",
        "\n",
        "<br>\n",
        "\n",
        "## Additional material: A Pytorch Module for Regression\n",
        "\n",
        "Pytorch offers a number of modules pre-implemented, many of which we can use for basic and deep neural networks.  \n",
        "Here we will use a simple linear module to perform linear regression of a dataset of points with Gaussian noise.\n",
        "\n",
        "- 3.1: Use the provided pseudo-code to implement linear regression using gradient descent and pytorch's autograd functionality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3YiM0Lm9Nes"
      },
      "source": [
        "### 3.0: The provided linear regression dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHEyx1dZ9Nes"
      },
      "source": [
        "set_seed(42)\n",
        "x = np.linspace(-1, 1, 100)\n",
        "y = x + np.random.normal(0, 0.25, size=(100))\n",
        "\n",
        "x, y = torch.from_numpy(x).float(), torch.from_numpy(y).float()\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
        "ax.scatter(x, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBDgg4fR9Nev"
      },
      "source": [
        "### 3.1: Use the provided pseudo-code to implement linear regression using gradient descent and pytorch's autograd functionality."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pf_Jiuhq9Nev"
      },
      "source": [
        "model = nn.Linear(1, 1, bias=True)\n",
        "print([(parameter, parameter.size())for parameter in model.parameters()]) # slope term and bias term (2 parameters)\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
        "criterion = torch.nn.MSELoss() # now we are defining a loss function we want to minimise\n",
        "## Pseudo-Code\n",
        "#iterate over the number of epochs (full data passes)\n",
        "    #iterate over all the data points\n",
        "        #reset optimizer gradients\n",
        "        #predict for current data point the y value ~ model(x)\n",
        "        #compute the loss by computing the criterion(y_prediction, y_ground_truth)\n",
        "        #perform backpropagation\n",
        "        #perform 1 optimizer step\n",
        "        \n",
        "#plot the regression line with the original data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLb9cZ_V9New"
      },
      "source": [
        "You can check the details of the torch.nn.Linear class documentation [here](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)\n",
        "\n",
        "and the documentation for the torch.nn.MSELoss class (Mean Squared Error, or squared L2 norm) [here](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11TR8Ftr9New"
      },
      "source": [
        "### Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dE-QJIAZ9New"
      },
      "source": [
        "model = nn.Linear(1, 1, bias=True)\n",
        "print([(parameter, parameter.size())for parameter in model.parameters()])\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
        "criterion = torch.nn.MSELoss() # now we are defining a loss function we want to minimise\n",
        "for iteration in range(100): #iterate over the number of epochs (full data passes)\n",
        "    total_loss = 0.\n",
        "    for i, point, value in zip(range(len(x)), x, y): #iterate over all the data points\n",
        "        optimizer.zero_grad()                        #reset optimizer gradients\n",
        "        y_ = model(point.view(1, 1))                 #predict for current data point the y value ~ model(x)\n",
        "        loss = criterion(y_, value.view(1, 1))       #compute the loss by computing the criterion(y_prediction, y_ground_truth)\n",
        "        loss.backward()                              #perform backpropagation\n",
        "        optimizer.step()                             #perform 1 optimizer step\n",
        "        total_loss += loss.item()\n",
        "    if iteration % 10 == 9 or iteration == 0:\n",
        "        print(\"Iteration: \", iteration, \"Weight: \", model.weight.data, \" Bias: \", model.bias.data, \" Loss: \", total_loss/x.size(0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jeVrYlA9Nex"
      },
      "source": [
        "Why the values of the weights and biases change more significantly than the loss?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wuso2Fz79Ney"
      },
      "source": [
        "predictions = [model(point.view(1, 1)) for point in x]\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
        "ax.scatter(x, y)\n",
        "ax.plot(x.numpy(), (model.weight.data.numpy()*x.numpy()+model.bias.data.numpy())[0], c=\"black\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xt4D66jpWtB1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}